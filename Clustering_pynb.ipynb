{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n",
        "\n",
        "# Theoretical Questions:\n",
        "\n",
        "1.  What is unsupervised learning in the context of machine learning?\n",
        "\n",
        "    - Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, meaning there are no predefined target outputs. The goal is to discover hidden patterns, groupings, or structures within the data. For example, clustering similar items or reducing data dimensions.\n",
        "\n",
        "\n",
        "2. How does K-Means clustering algorithm work?\n",
        "\n",
        "   - K-Means divides data into K clusters by minimizing the variance within each cluster.\n",
        "    1. Choose the number of clusters (K).\n",
        "    2. Randomly initialize K centroids.\n",
        "    3. Assign each data point to the nearest centroid.\n",
        "    4. Recalculate centroids as the mean of points in each cluster.\n",
        "    5. Repeat steps 3 or 4 until centroids no longer change.\n",
        "\n",
        "\n",
        "3. Explain the concept of a dendrogram in hierarchical clustering.\n",
        "\n",
        "   - A dendrogram is a tree-like diagram that shows how data points are merged or split in hierarchical clustering. The height of each merge represents the distance or dissimilarity between clusters, helping to visually decide the optimal number of clusters.\n",
        "\n",
        "\n",
        "4. What is the main difference between K-Means and Hierarchical Clustering?\n",
        "\n",
        "   - K-Means: Requires the number of clusters (K) in advance; partitions data flatly.\n",
        "   - Hierarchical: Builds a hierarchy of clusters without predefining K; visualized via dendrogram.\n",
        "   - K-Means is faster, while hierarchical clustering provides more interpretability.\n",
        "\n",
        "\n",
        "5. What are the advantages of DBSCAN over K-Means?\n",
        "\n",
        "   - The advantages of DBSCAN over K-Means are:\n",
        "   1. Detects arbitrary and shaped clusters.\n",
        "   2. Automatically finds the number of clusters.\n",
        "   3. Can detect noise and outliers.\n",
        "   4. Doesn't assume clusters are spherical, unlike K-Means.\n",
        "\n",
        "\n",
        "6. When would you use Silhouette Score in clustering?\n",
        "\n",
        "   - The Silhouette Score is a crucial metric used in clustering to evaluate the quality and consistency of the resulting clusters. It is an unsupervised evaluation metric, meaning it doesn't require the true cluster labels of the data.\n",
        "   1. Choosing the Optimal Number of Clusters (k): The most common use of the Silhouette Score is to help determine the best number of clusters (k) for algorithms like K-Means or K-Medoids.\n",
        "   2. Evaluating and Comparing Cluster Quality: The score provides a single, easy-to-interpret number to assess how well-defined the clusters are.\n",
        "   3. Identifying Outliers and Poorly Clustered Points: The score is calculated for each individual data point, which allows for a more granular analysis.\n",
        "\n",
        "\n",
        "7. What are the limitations of Hierarchical Clustering?\n",
        "\n",
        "   - The limitations of Hierarchical Clustering (HC) primarily stem from its computational intensity and the permanent nature of its decisions.\n",
        "     - High Computational and Memory Cost.\n",
        "     - Irreversible Decisions.\n",
        "     - Sensitivity to Input Parameters.\n",
        "     - Difficulty in Handling Noise and Outliers.\n",
        "     - Difficulty in Determining the Optimal Number of Clusters.\n",
        "\n",
        "\n",
        "8. Why is feature scaling important in clustering algorithms like K-Means?\n",
        "\n",
        "   - K-Means uses Euclidean distance to measure similarity. If features have different scales, those with larger ranges dominate the distance metric, leading to biased clusters. Scaling ensures all features contribute equally.\n",
        "     - Prevent Feature Dominance.\n",
        "     - Ensure Equal Contribution.\n",
        "     - Faster Convergence.\n",
        "\n",
        "\n",
        "9. How does DBSCAN identify noise points?\n",
        "\n",
        "   - DBSCAN classifies points as:\n",
        "   - Core points: Have at least min_samples neighbors within radius eps.\n",
        "   - Border points: Near core points but have fewer neighbors.\n",
        "   - Noise points: Do not satisfy either condition.\n",
        "\n",
        "\n",
        "10. Define inertia in the context of K-Means.\n",
        "\n",
        "    - Inertia is the sum of squared distances between each point and its assigned cluster centroid.\n",
        "    - Lower inertia means tighter clusters, but it should be balanced to avoid overfitting.\n",
        "    - The primary objective of the K-Means algorithm is to minimize the inertia. A lower inertia score means that the data points within each cluster are closer to their respective centroids, indicating denser and more tightly bound clusters.\n",
        "\n",
        "\n",
        "11. What is the elbow method in K-Means clustering?\n",
        "\n",
        "    - The Elbow Method is a popular heuristic technique used to determine the optimal number of clusters (k) for the K-Means clustering algorithm.\n",
        "    - It works by plotting a measure of cluster quality against the number of clusters and looking for a point on the graph where the rate of improvement sharply decreases, resembling an elbow joint.\n",
        "\n",
        "\n",
        "12. Describe the concept of \"density\" in DBSCAN.\n",
        "\n",
        "    - The concept of \"density\" is fundamental to the Density-Based Spatial Clustering of Applications with Noise algorithm. Unlike K-Means, which uses distance to centroids, DBSCAN defines clusters as contiguous regions of high density separated by regions of low density.\n",
        "    - In DBSCAN, density refers to how closely packed points are in a region. A dense region with at least min_samples points within a radius eps forms a cluster.\n",
        "\n",
        "\n",
        "13. Can hierarchical clustering be used on categorical data?\n",
        "\n",
        "    - Yes, hierarchical clustering can be used on categorical data, but it requires using specialized dissimilarity/distance measures instead of standard metrics like Euclidean distance.\n",
        "    - Hierarchical clustering, fundamentally, relies on a distance matrix between all pairs of data points. For the algorithm to work with categorical data, you simply need a metric that can accurately quantify the \"distance\" or difference between two categorical records.\n",
        "\n",
        "\n",
        "14. What does a negative Silhouette Score indicate?\n",
        "\n",
        "    - A negative Silhouette Score means that points are misclassified, i.e., they are closer to another cluster than their assigned one. It indicates poor clustering quality.\n",
        "    - The Silhouette Score (s) for a single data point is calculated using two values:\n",
        "    1. Cohesion (a): The average distance of the point to all other points in its own cluster. A smaller $a$ indicates better cohesion.\n",
        "    2. Separation (b): The minimum average distance of the point to all points in any other cluster. A larger b indicates better separation.\n",
        "\n",
        "\n",
        "15. Explain the term \"linkage criteria\" in hierarchical clustering.\n",
        "\n",
        "    - Linkage criteria in hierarchical clustering define the distance between two clusters or groups of data points. Since the hierarchical process involves successively merging or splitting clusters, a rule is needed to calculate the separation between these multi-point groups so the algorithm knows which two clusters are the \"closest\" and should be merged next or how to define the distance for splitting.\n",
        "\n",
        "      - Single linkage: Minimum distance between points.\n",
        "      - Complete linkage: Maximum distance between points.\n",
        "      - Average linkage: Mean distance between all pairs of points.\n",
        "      - Ward's linkage: Minimizes variance within clusters.\n",
        "\n",
        "\n",
        "16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
        "\n",
        "    - K-Means clustering often performs poorly on data with varying cluster sizes or densities because its underlying mechanism and objective function are optimized for finding spherical, equally sized, and equally dense clusters.\n",
        "    1. The Dependence on Mean: K-Means works by minimizing the Within-Cluster Sum of Squares, which essentially tries to make all points close to their cluster's mean.\n",
        "    2. The Spherical Assumption: K-Means inherently assumes that clusters are convex and roughly spherical because it uses the Euclidean distance metric and assigns points to the nearest centroid\n",
        "    3. The Objective Function Bias: In a set of clusters with widely varying densities, the algorithm seeks to reduce the overall variance.\n",
        "\n",
        "\n",
        "17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "\n",
        "    - The DBSCAN algorithm is controlled by two core parameters that define the concept of density and, consequently, the resulting clusters.\n",
        "    1. Epsilon or eps: A distance threshold that defines the radius of the neighborhood around a given data point.\n",
        "       - Influence: It determines how close points must be to each other to be considered part of the same cluster.\n",
        "    2. Minimum Points: The minimum number of data points required to be present within the epsilon-neighborhood of a point for that point to be classified as a Core Point.\n",
        "       - Influence: It defines the minimum required density for a region to be considered a cluster.\n",
        "\n",
        "\n",
        "18. How does K-Means++ improve upon standard K-Means initialization?\n",
        "\n",
        "    - K-Means++ dramatically improves upon standard K-Means initialization by using a smart, probabilistic approach to select initial cluster centers that are well-separated.\n",
        "    - K-Means++ selects initial centroids smartly to spread them apart, reducing chances of poor convergence and speeding up the algorithm. It often leads to better clustering stability and lower inertia.\n",
        "\n",
        "19. What is agglomerative clustering?\n",
        "\n",
        "    - Agglomerative clustering is a bottom-up hierarchical approach.Each point starts as its own cluster, and pairs of clusters are merged iteratively based on distance until one cluster or the desired number remain.\n",
        "    \n",
        "    - Agglomerative Clustering Works:\n",
        "    1. Initialization: Start by treating each data point as a single cluster.\n",
        "    2. Distance Calculation: Compute the distance between all pairs of clusters using a chosen distance metric.\n",
        "    3. Merging: Merge the two closest clusters into a new, single cluster.\n",
        "    4. Update: Recalculate the distances between the new cluster and all the remaining clusters.\n",
        "    5. Iteration: Repeat steps 2-4 until all data points are merged into one large cluster.\n",
        "\n",
        "\n",
        "20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "\n",
        "    - Inertia only measures compactness, not separation between clusters.\n",
        "    - Silhouette Score evaluates both compactness and separation, providing a more comprehensive measure of clustering quality.\n",
        "\n",
        "\n",
        "# Practical Questions:\n",
        "\n",
        "21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import KMeans\n",
        "      import matplotlib.pyplot as plt\n",
        "\n",
        "      X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "      kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "      labels = kmeans.fit_predict(X)\n",
        "\n",
        "      plt.scatter(X[:,0], X[:,1], c=labels, cmap='rainbow')\n",
        "      plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='black', marker='X', s=200)\n",
        "      plt.title(\"K-Means Clustering with 4 Centers\")\n",
        "      plt.show()\n",
        "\n",
        "22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n",
        "\n",
        "- Answer\n",
        "      \n",
        "      from sklearn.datasets import load_breast_cancer\n",
        "      from sklearn.decomposition import PCA\n",
        "      from sklearn.cluster import AgglomerativeClustering\n",
        "      import matplotlib.pyplot as plt\n",
        "      %matplotlib inline\n",
        "\n",
        "      data = load_breast_cancer()\n",
        "      X = data.data\n",
        "      pca = PCA(n_components=2, random_state=42)\n",
        "      X2 = pca.fit_transform(X)\n",
        "\n",
        "      agg = AgglomerativeClustering(n_clusters=2, linkage='average')\n",
        "      labels = agg.fit_predict(X2)\n",
        "\n",
        "      plt.scatter(X2[:,0], X2[:,1], c=labels, cmap='coolwarm', s=30)\n",
        "      plt.title(\"Agglomerative Clustering on Breast Cancer (PCA 2D)\")\n",
        "      plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "23. Generate synthetic data using make_moons and apply DBSCAN.Highlight outliers in the plot.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_circles\n",
        "      from sklearn.cluster import KMeans, DBSCAN\n",
        "      import matplotlib.pyplot as plt\n",
        "      %matplotlib inline\n",
        "\n",
        "      X, y = make_circles(n_samples=500, factor=0.5, noise=0.07, random_state=0)\n",
        "\n",
        "      kmeans = KMeans(n_clusters=2, random_state=0, n_init=10).fit_predict(X)\n",
        "      db = DBSCAN(eps=0.15, min_samples=5).fit_predict(X)\n",
        "\n",
        "      fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
        "      axes[0].scatter(X[:,0], X[:,1], c=kmeans, s=25)\n",
        "      axes[0].set_title(\"KMeans (k=2)\")\n",
        "      axes[1].scatter(X[:,0], X[:,1], c=db, s=25)\n",
        "      axes[1].set_title(\"DBSCAN (eps=0.15)\")\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      import numpy as np\n",
        "      import matplotlib.pyplot as plt\n",
        "      from sklearn.datasets import load_iris\n",
        "      from sklearn.cluster import KMeans\n",
        "      from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "      %matplotlib inline\n",
        "\n",
        "      iris = load_iris()\n",
        "      X = iris.data\n",
        "      kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "      labels = kmeans.fit_predict(X)\n",
        "      avg_score = silhouette_score(X, labels)\n",
        "      sample_scores = silhouette_samples(X, labels)\n",
        "\n",
        "      print(\"Average Silhouette Score:\", avg_score)\n",
        "\n",
        "      # Plotting per-sample silhouette\n",
        "      y_lower = 10\n",
        "      fig, ax = plt.subplots(figsize=(7,5))\n",
        "      for i in range(3):\n",
        "          ith_scores = sample_scores[labels == i]\n",
        "          ith_scores.sort()\n",
        "          size = ith_scores.shape[0]\n",
        "          y_upper = y_lower + size\n",
        "          ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_scores)\n",
        "          ax.text(-0.05, y_lower + 0.5 * size, str(i))\n",
        "          y_lower = y_upper + 10\n",
        "      ax.set_title(f\"Silhouette plot (avg={avg_score:.3f})\")\n",
        "      ax.set_xlabel(\"Silhouette coefficient\")\n",
        "      ax.set_ylabel(\"Cluster label\")\n",
        "      plt.show()\n",
        "\n",
        "25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import AgglomerativeClustering\n",
        "      import matplotlib.pyplot as plt\n",
        "      %matplotlib inline\n",
        "\n",
        "      X, y = make_blobs(n_samples=400, centers=4, cluster_std=0.6, random_state=7)\n",
        "      agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "      labels = agg.fit_predict(X)\n",
        "\n",
        "      plt.scatter(X[:,0], X[:,1], c=labels, s=30, cmap='tab10')\n",
        "      plt.title(\"Agglomerative Clustering (average linkage)\")\n",
        "      plt.show()\n",
        "\n",
        "26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      import seaborn as sns\n",
        "      import pandas as pd\n",
        "      from sklearn.datasets import load_wine\n",
        "      from sklearn.cluster import KMeans\n",
        "      %matplotlib inline\n",
        "\n",
        "      wine = load_wine()\n",
        "      df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "      kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "      df['cluster'] = kmeans.fit_predict(df[wine.feature_names])\n",
        "      sns.pairplot(df.iloc[:, :4].assign(cluster=df['cluster']), hue='cluster', diag_kind='kde', corner=False)\n",
        "      plt.suptitle(\"Wine dataset pairplot (first 4 features) with KMeans clusters\", y=1.02)\n",
        "      plt.show()\n",
        "\n",
        "27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import DBSCAN\n",
        "      import numpy as np\n",
        "\n",
        "      X, y = make_blobs(n_samples=500, centers=3, cluster_std=[0.4, 0.8, 1.5], random_state=42)\n",
        "      # Add scattered noise\n",
        "      rng = np.random.RandomState(1)\n",
        "      noise = rng.uniform(low=-8, high=8, size=(30, 2))\n",
        "      X = np.vstack([X, noise])\n",
        "\n",
        "      db = DBSCAN(eps=0.6, min_samples=5).fit(X)\n",
        "      labels = db.labels_\n",
        "      n_noise = np.sum(labels == -1)\n",
        "      n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "      print(\"Clusters found (excluding noise):\", n_clusters)\n",
        "      print(\"Noise points:\", n_noise)\n",
        "\n",
        "28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_digits\n",
        "      from sklearn.decomposition import PCA\n",
        "\n",
        "      digits = load_digits()\n",
        "      pca = PCA(n_components=2)\n",
        "      X_pca = pca.fit_transform(digits.data)\n",
        "\n",
        "      kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "      labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "      plt.scatter(X_pca[:,0], X_pca[:,1], c=labels, cmap='tab10')\n",
        "      plt.title(\"K-Means Clustering on Digits (2D PCA Projection)\")\n",
        "      plt.show()\n",
        "\n",
        "29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.metrics import silhouette_score\n",
        "\n",
        "      X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "      scores = []\n",
        "      for k in range(2, 6):\n",
        "          kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "          labels = kmeans.fit_predict(X)\n",
        "          scores.append(silhouette_score(X, labels))\n",
        "\n",
        "      plt.bar(range(2, 6), scores, color='skyblue')\n",
        "      plt.xlabel('Number of Clusters (k)')\n",
        "      plt.ylabel('Silhouette Score')\n",
        "      plt.title('Silhouette Scores for k = 2 to 5')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "      X = load_iris().data\n",
        "      Z = linkage(X, method='average')\n",
        "      plt.figure(figsize=(8, 4))\n",
        "      dendrogram(Z)\n",
        "      plt.title(\"Dendrogram (Average Linkage) - Iris Dataset\")\n",
        "      plt.xlabel(\"Samples\")\n",
        "      plt.ylabel(\"Distance\")\n",
        "      plt.show()    \n",
        "\n",
        "31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import KMeans\n",
        "      import numpy as np\n",
        "      import matplotlib.pyplot as plt\n",
        "\n",
        "      X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.0, random_state=42)\n",
        "      kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
        "      labels = kmeans.labels_\n",
        "\n",
        "      # Decision boundary\n",
        "      x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "      y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "      xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
        "      Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "      plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "      plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "      plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200)\n",
        "      plt.title(\"K-Means Decision Boundaries with Overlapping Clusters\")\n",
        "      plt.show()      \n",
        "\n",
        "32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_digits\n",
        "      from sklearn.manifold import TSNE\n",
        "      from sklearn.cluster import DBSCAN\n",
        "\n",
        "      digits = load_digits()\n",
        "      X_tsne = TSNE(n_components=2, random_state=42).fit_transform(digits.data)\n",
        "      db = DBSCAN(eps=5, min_samples=5).fit(X_tsne)\n",
        "      labels = db.labels_\n",
        "\n",
        "      plt.scatter(X_tsne[:,0], X_tsne[:,1], c=labels, cmap='tab10', s=10)\n",
        "      plt.title(\"DBSCAN Clustering on Digits (t-SNE Reduced Data)\")\n",
        "      plt.show()\n",
        "\n",
        "33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "      X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "      agg = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "      labels = agg.fit_predict(X)\n",
        "\n",
        "      plt.scatter(X[:,0], X[:,1], c=labels, cmap='rainbow')\n",
        "      plt.title(\"Agglomerative Clustering (Complete Linkage)\")\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_breast_cancer\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "      data = load_breast_cancer()\n",
        "      scaler = StandardScaler()\n",
        "      X_scaled = scaler.fit_transform(data.data)\n",
        "\n",
        "      inertias = []\n",
        "      for k in range(2, 7):\n",
        "          kmeans = KMeans(n_clusters=k, random_state=42).fit(X_scaled)\n",
        "          inertias.append(kmeans.inertia_)\n",
        "\n",
        "      plt.plot(range(2, 7), inertias, marker='o')\n",
        "      plt.title(\"K-Means Inertia Values for K = 2 to 6 (Breast Cancer Data)\")\n",
        "      plt.xlabel(\"Number of Clusters (k)\")\n",
        "      plt.ylabel(\"Inertia\")\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_circles\n",
        "\n",
        "      X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
        "      agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "      labels = agg.fit_predict(X)\n",
        "\n",
        "      plt.scatter(X[:,0], X[:,1], c=labels, cmap='rainbow')\n",
        "      plt.title(\"Agglomerative Clustering (Single Linkage) on make_circles\")\n",
        "      plt.show()\n",
        "\n",
        "36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise).\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_wine\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      from sklearn.cluster import DBSCAN\n",
        "      import numpy as np\n",
        "\n",
        "      wine = load_wine()\n",
        "      scaler = StandardScaler()\n",
        "      X_scaled = scaler.fit_transform(wine.data)\n",
        "\n",
        "      db = DBSCAN(eps=1.5, min_samples=5).fit(X_scaled)\n",
        "      labels = db.labels_\n",
        "      n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "      print(\"Number of clusters (excluding noise):\", n_clusters)\n",
        "\n",
        "\n",
        "37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import KMeans\n",
        "      import matplotlib.pyplot as plt\n",
        "\n",
        "      X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "      kmeans = KMeans(n_clusters=4, random_state=42).fit(X)\n",
        "      labels = kmeans.labels_\n",
        "\n",
        "      plt.scatter(X[:,0], X[:,1], c=labels, cmap='rainbow')\n",
        "      plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='black', s=200, marker='X')\n",
        "      plt.title(\"KMeans Clustering with Cluster Centers\")\n",
        "      plt.show()\n",
        "\n",
        "38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_iris\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      from sklearn.cluster import DBSCAN\n",
        "\n",
        "      iris = load_iris()\n",
        "      X_scaled = StandardScaler().fit_transform(iris.data)\n",
        "\n",
        "      db = DBSCAN(eps=0.8, min_samples=5).fit(X_scaled)\n",
        "      labels = db.labels_\n",
        "      n_noise = list(labels).count(-1)\n",
        "      print(\"Number of noise samples identified by DBSCAN:\", n_noise)\n",
        "\n",
        "39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_moons\n",
        "      from sklearn.cluster import KMeans\n",
        "\n",
        "      X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "      kmeans = KMeans(n_clusters=2, random_state=42).fit(X)\n",
        "      labels = kmeans.labels_\n",
        "\n",
        "      plt.scatter(X[:,0], X[:,1], c=labels, cmap='coolwarm')\n",
        "      plt.title(\"KMeans Clustering on make_moons (Non-linear Data)\")\n",
        "      plt.show()\n",
        "\n",
        "40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_digits\n",
        "      from sklearn.decomposition import PCA\n",
        "      from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "      digits = load_digits()\n",
        "      pca = PCA(n_components=3)\n",
        "      X_pca = pca.fit_transform(digits.data)\n",
        "\n",
        "      kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "      labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "      fig = plt.figure(figsize=(8,6))\n",
        "      ax = fig.add_subplot(111, projection='3d')\n",
        "      ax.scatter(X_pca[:,0], X_pca[:,1], X_pca[:,2], c=labels, cmap='tab10', s=15)\n",
        "      ax.set_title(\"3D Visualization of KMeans Clustering on Digits (PCA Reduced)\")\n",
        "      plt.show()\n",
        "\n",
        "41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import KMeans\n",
        "      from sklearn.metrics import silhouette_score\n",
        "\n",
        "      X, _ = make_blobs(n_samples=400, centers=5, random_state=42)\n",
        "      kmeans = KMeans(n_clusters=5, random_state=42).fit(X)\n",
        "      labels = kmeans.labels_\n",
        "\n",
        "      score = silhouette_score(X, labels)\n",
        "      print(\"Silhouette Score for KMeans with 5 centers:\", score)\n",
        "\n",
        "\n",
        "42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_breast_cancer\n",
        "      from sklearn.decomposition import PCA\n",
        "      from sklearn.cluster import AgglomerativeClustering\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      import matplotlib.pyplot as plt\n",
        "\n",
        "      data = load_breast_cancer()\n",
        "      X_scaled = StandardScaler().fit_transform(data.data)\n",
        "      X_pca = PCA(n_components=2).fit_transform(X_scaled)\n",
        "\n",
        "      agg = AgglomerativeClustering(n_clusters=2)\n",
        "      labels = agg.fit_predict(X_pca)\n",
        "\n",
        "      plt.scatter(X_pca[:,0], X_pca[:,1], c=labels, cmap='rainbow')\n",
        "      plt.title(\"Agglomerative Clustering on Breast Cancer (PCA Reduced)\")\n",
        "      plt.show()\n",
        "\n",
        "43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_circles\n",
        "      from sklearn.cluster import DBSCAN\n",
        "      import matplotlib.pyplot as plt\n",
        "\n",
        "      X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "      # KMeans\n",
        "      kmeans = KMeans(n_clusters=2, random_state=42).fit(X)\n",
        "      labels_kmeans = kmeans.labels_\n",
        "\n",
        "      # DBSCAN\n",
        "      db = DBSCAN(eps=0.1, min_samples=5).fit(X)\n",
        "      labels_dbscan = db.labels_\n",
        "\n",
        "      fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "      axes[0].scatter(X[:,0], X[:,1], c=labels_kmeans, cmap='rainbow')\n",
        "      axes[0].set_title(\"KMeans on make_circles\")\n",
        "      axes[1].scatter(X[:,0], X[:,1], c=labels_dbscan, cmap='rainbow')\n",
        "      axes[1].set_title(\"DBSCAN on make_circles\")\n",
        "      plt.show()\n",
        "\n",
        "44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_iris\n",
        "      from sklearn.metrics import silhouette_samples\n",
        "      import numpy as np\n",
        "\n",
        "      iris = load_iris()\n",
        "      X = iris.data\n",
        "      kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
        "      labels = kmeans.labels_\n",
        "\n",
        "      sil_samples = silhouette_samples(X, labels)\n",
        "      plt.bar(range(len(sil_samples)), sil_samples, color='skyblue')\n",
        "      plt.title(\"Silhouette Coefficient for Each Sample (Iris Dataset)\")\n",
        "      plt.xlabel(\"Sample Index\")\n",
        "      plt.ylabel(\"Silhouette Coefficient\")\n",
        "      plt.show()\n",
        "\n",
        "45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "      X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "      agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "      labels = agg.fit_predict(X)\n",
        "\n",
        "      plt.scatter(X[:,0], X[:,1], c=labels, cmap='rainbow')\n",
        "      plt.title(\"Agglomerative Clustering (Average Linkage)\")\n",
        "      plt.show()\n",
        "\n",
        "46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)\n",
        "\n",
        "- Answer\n",
        "      \n",
        "      from sklearn.datasets import load_wine\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      import seaborn as sns\n",
        "      import pandas as pd\n",
        "\n",
        "      wine = load_wine()\n",
        "      scaler = StandardScaler()\n",
        "      X_scaled = scaler.fit_transform(wine.data[:, :4])\n",
        "\n",
        "      kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "      labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "      df = pd.DataFrame(X_scaled, columns=wine.feature_names[:4])\n",
        "      df['Cluster'] = labels\n",
        "      sns.pairplot(df, hue='Cluster', palette='tab10')\n",
        "      plt.suptitle(\"KMeans Clustering on Wine Data (First 4 Features)\", y=1.02)\n",
        "      plt.show()\n",
        "\n",
        "47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import make_blobs\n",
        "      from sklearn.cluster import DBSCAN\n",
        "      import numpy as np\n",
        "\n",
        "      X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.0, random_state=42)\n",
        "      db = DBSCAN(eps=1.0, min_samples=5).fit(X)\n",
        "      labels = db.labels_\n",
        "\n",
        "      n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "      n_noise = list(labels).count(-1)\n",
        "\n",
        "      print(\"Number of clusters:\", n_clusters)\n",
        "      print(\"Number of noise points:\", n_noise)\n",
        "\n",
        "48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from sklearn.datasets import load_digits\n",
        "      from sklearn.manifold import TSNE\n",
        "      from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "      digits = load_digits()\n",
        "      X_tsne = TSNE(n_components=2, random_state=42).fit_transform(digits.data)\n",
        "\n",
        "      agg = AgglomerativeClustering(n_clusters=10)\n",
        "      labels = agg.fit_predict(X_tsne)\n",
        "\n",
        "      plt.scatter(X_tsne[:,0], X_tsne[:,1], c=labels, cmap='tab10', s=10)\n",
        "      plt.title(\"Agglomerative Clustering on Digits Dataset (t-SNE Reduced)\")\n",
        "      plt.show()\n"
      ],
      "metadata": {
        "id": "oRB68RTDVzcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "je3grdpyu2OO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}